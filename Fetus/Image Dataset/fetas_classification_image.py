# -*- coding: utf-8 -*-
"""Fetas Classification Image.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWqyv2UtLk7y9lwg9dc4Wak1bfEATkFK
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Navigate to the directory where your dataset is stored in Google Drive
data_dir = '/content/drive/MyDrive/MLSA BUP/Classification'

import pandas as pd

# Load the CSV file
csv_path = f'{data_dir}/image_label.csv'
dataset = pd.read_csv(csv_path)

# Display the first few rows and class distribution
print(dataset.head())
print("Class Distribution:\n", dataset['Plane'].value_counts())

import os
import cv2
import numpy as np

# Directory where images are stored
image_dir = f'{data_dir}/images'

# Target image size (adjust as needed)
img_size = (256, 256)

# Encode labels as integers
label_mapping = {label: idx for idx, label in enumerate(dataset['Plane'].unique())}
dataset['Label'] = dataset['Plane'].map(label_mapping)

# Load images and labels as arrays
def load_data(dataset, image_dir):
    images = []
    labels = []
    for _, row in dataset.iterrows():
        image_path = os.path.join(image_dir, f"{row['Image_name']}.png")
        if os.path.exists(image_path):
            # Load and resize the image
            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            img_resized = cv2.resize(img, img_size)
            images.append(img_resized)
            labels.append(row['Label'])
        else:
            print(f"Image not found: {image_path}")
    return np.array(images), np.array(labels)

# Load the dataset
images, labels = load_data(dataset, image_dir)

# Reshape and normalize the images
images = images[..., np.newaxis] / 255.0  # Add channel dimension and normalize

from sklearn.model_selection import train_test_split

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)

# Display the shape of the datasets
print("Training data shape:", X_train.shape)
print("Validation data shape:", X_val.shape)

import tensorflow as tf
from tensorflow.keras import layers, models

# Define the CNN model with increased complexity
def create_model(input_shape=(256, 256, 1), num_classes=num_classes):
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(256, (3, 3), activation='relu'),  # Additional convolutional layer
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])
    return model

# Instantiate and compile the model with a reduced learning rate
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)  # Reduced learning rate
model = create_model()
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Display model summary
model.summary()

from sklearn.utils import class_weight

# Calculate class weights based on label distribution
class_weights = class_weight.compute_class_weight(
    'balanced', classes=np.unique(labels), y=labels
)
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}

print("Class Weights:", class_weight_dict)

# Define model checkpoint path
checkpoint_path = '/content/drive/MyDrive/MLSA BUP/best_model.keras'  # Ensure `.keras` extension

# Define callbacks
callbacks = [
    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_accuracy', mode='max'),
    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),  # Increased patience
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)  # Learning rate scheduler
]

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Initialize ImageDataGenerator for data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
)

# Define the training generator with augmented data
train_generator = datagen.flow(X_train, y_train, batch_size=32)

# Train the model with data augmentation and updated callbacks
history = model.fit(train_generator, epochs=20, validation_data=(X_val, y_val),
                    class_weight=class_weight_dict, callbacks=callbacks)

# Evaluate the model on validation data
val_loss, val_accuracy = model.evaluate(X_val, y_val)
print(f"Validation Accuracy: {val_accuracy * 100:.2f}%")

import matplotlib.pyplot as plt

# Plot training & validation accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')

# Plot training & validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')

plt.show()

import os
import cv2
import numpy as np

# Define the test images directory
test_image_dir = '/content/drive/MyDrive/MLSA BUP/Classification/External Test images'
img_size = (256, 256)  # Ensure this matches the input size used during training

# Function to load and preprocess test images
def load_test_images(image_dir):
    images = []
    image_names = []
    for img_name in os.listdir(image_dir):
        img_path = os.path.join(image_dir, img_name)
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is not None:
            img_resized = cv2.resize(img, img_size) / 255.0  # Normalize
            img_resized = np.expand_dims(img_resized, axis=-1)  # Add channel dimension
            images.append(img_resized)
            image_names.append(img_name)
    return np.array(images), image_names

# Load the test images
X_test, test_image_names = load_test_images(test_image_dir)

print("Loaded test images:", len(X_test))

# Predict on test images
predictions = model.predict(X_test)

# Decode predictions into class labels
predicted_labels = [list(label_mapping.keys())[np.argmax(pred)] for pred in predictions]

import matplotlib.pyplot as plt

# Display test images with predicted labels
for i in range(len(X_test)):
    plt.imshow(X_test[i].reshape(256, 256), cmap='gray')
    plt.title(f"Predicted: {predicted_labels[i]}")
    plt.axis('off')
    plt.show()

from collections import defaultdict

# Group images by predicted class
grouped_predictions = defaultdict(list)
for i, label in enumerate(predicted_labels):
    grouped_predictions[label].append(test_image_names[i])

# Display the grouped results
for label, images in grouped_predictions.items():
    print(f"\nPredicted Class: {label}")
    print("Images:", images[:10])  # Display only the first 10 image names for brevity

# Display predictions with confidence scores
for i, pred in enumerate(predictions):
    confidence = np.max(pred)  # Confidence score of the top prediction
    predicted_label = predicted_labels[i]
    print(f"Image: {test_image_names[i]}, Predicted: {predicted_label}, Confidence: {confidence:.2f}")







